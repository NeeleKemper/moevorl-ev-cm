# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ddpg/#ddpg_continuous_actionpy
import os
from copy import deepcopy

import wandb
import random
from typing import List, Optional, Union

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from morl.common.prioritized_buffer import PrioritizedReplayBuffer
from envs.ev_charging.ev_charging import EVCharging
from morl.common.evaluation import seed_everything
from morl.common.evaluation import (
    log_episode_info,
)
from morl.common.weight_sampler import RandomWeightSampler
from morl.common.morl_algorithm import MOAgent, MOPolicy
from morl.common.networks import layer_init, mlp
from morl.common.weights import equally_spaced_weights


class OrnsteinUhlenbeckNoise:
    def __init__(self, size: int, theta=0.15, sigma=0.05, ou_noise=0.0, dt=1e-2):
        """
        The noise of Ornstein-Uhlenbeck Process

        Source: https://github.com/slowbull/DDPG/blob/master/src/explorationnoise.py
        It makes Zero-mean Gaussian Noise more stable.
        It helps agent explore better in a inertial system.
        Don't abuse OU Process. OU process has too much hyper-parameters and over fine-tuning make no sense.

        int size: the size of noise, noise.shape==(-1, action_dim)
        float theta: related to the not independent of OU-noise
        float sigma: related to action noise std
        float ou_noise: initialize OU-noise
        float dt: derivative
        """
        self.theta = theta
        self.sigma = sigma
        self.ou_noise = ou_noise
        self.dt = dt
        self.size = size

    def sample(self) -> float:
        """
        output a OU-noise

        return array ou_noise: a noise generated by Ornstein-Uhlenbeck Process
        """
        noise = self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)
        self.ou_noise -= self.theta * self.ou_noise * self.dt + noise
        return self.ou_noise


# ALGO LOGIC: initialize agent here:
class QNetwork(nn.Module):
    def __init__(self, obs_dim: int, action_dim: int, rew_dim: int, net_arch=[256, 256]):
        super().__init__()
        self.net = mlp(obs_dim + action_dim + rew_dim, rew_dim, net_arch)
        self.apply(layer_init)

    def forward(self, obs, action, w):
        return self.net(torch.cat((obs, action, w), dim=obs.dim() - 1))


class Actor(nn.Module):
    def __init__(self, obs_dim: int, action_dim: int, reward_dim: int, action_space: any, net_arch=[256, 256]):
        super().__init__()
        self.net = mlp(obs_dim + reward_dim, action_dim, net_arch)

        # action rescaling
        self.register_buffer(
            "action_scale", torch.tensor((action_space.high - action_space.low) / 2.0, dtype=torch.float32)
        )
        self.register_buffer(
            "action_bias", torch.tensor((action_space.high + action_space.low) / 2.0, dtype=torch.float32)
        )

    def forward(self, obs, w):
        x = torch.concat((obs, w), dim=obs.dim() - 1)
        x = self.net(x)
        return torch.tanh(x) * self.action_scale + self.action_bias

    def get_action(self, obs, w):
        """Get an action from the policy network."""
        return self.forward(obs, w)


class MODDPG(MOAgent, MOPolicy):
    def __init__(
            self,
            envs,
            learning_rate: float = 1e-4,
            gamma: float = 0.99,
            tau: float = 0.005,
            buffer_size: int = 1000000,
            net_arch: List = [512, 512],
            batch_size: int = 256,
            learning_starts: int = 1000,
            per_alpha: float = 0.6,
            policy_frequency: int = 1,
            env_iterations: int = 10,
            project_name: str = "MORL-Baselines",
            experiment_name: str = "MO_DDPG",
            wandb_entity: Optional[str] = None,
            log: bool = True,
            seed: Optional[int] = None,
            device: Union[torch.device, str] = "auto",
    ):
        if seed is not None:
            seed_everything(seed)
        self.rnd = random.Random(seed)

        MOAgent.__init__(self, envs[0], device=device, seed=seed)
        MOPolicy.__init__(self, device=device)
        self.envs = envs
        self.learning_rate = learning_rate
        self.tau = tau
        self.gamma = gamma
        self.buffer_size = buffer_size
        self.net_arch = net_arch
        self.learning_starts = learning_starts
        self.per_alpha = per_alpha
        self.batch_size = batch_size
        self.policy_frequency = policy_frequency
        self.env_iterations = env_iterations
        self.weight_sampler = None
        self.seed = seed

        # Q network
        self.qf1 = QNetwork(obs_dim=self.observation_dim, action_dim=self.action_dim, rew_dim=self.reward_dim,
                            net_arch=net_arch).to(self.device)
        self.qf1_target = QNetwork(obs_dim=self.observation_dim, action_dim=self.action_dim, rew_dim=self.reward_dim,
                                   net_arch=net_arch).to(self.device)
        self.qf1_target.load_state_dict(self.qf1.state_dict())

        # Actor network
        self.actor = Actor(obs_dim=self.observation_dim, action_dim=self.action_dim, reward_dim=self.reward_dim,
                           action_space=self.action_space, net_arch=net_arch).to(self.device)
        self.actor_target = Actor(obs_dim=self.observation_dim, action_dim=self.action_dim, reward_dim=self.reward_dim,
                                  action_space=self.action_space, net_arch=net_arch).to(self.device)
        self.actor_target.load_state_dict(self.actor.state_dict())

        # optimizer
        self.q_optimizer = optim.Adam(list(self.qf1.parameters()), lr=self.learning_rate)
        self.actor_optimizer = optim.Adam(list(self.actor.parameters()), lr=self.learning_rate)

        self.replay_buffer = PrioritizedReplayBuffer(self.observation_shape, self.action_dim, rew_dim=self.reward_dim,
                                                     max_size=self.buffer_size, obs_dtype=np.float32,
                                                     action_dtype=np.float32, min_priority=1e-5)

        # noise
        self.exploration_noise = OrnsteinUhlenbeckNoise(size=self.action_dim)

        # weight sampler
        self.weight_sampler = RandomWeightSampler(self.reward_dim, self.seed)

        # log
        self.log = log
        if self.log:
            self.setup_wandb(project_name, experiment_name, wandb_entity)

        self.prev_actor_loss = 0

    def _sample_batch_experiences(self):
        return self.replay_buffer.sample(self.batch_size, to_tensor=True, device=self.device)

    def update(self):
        with torch.no_grad():
            (s_obs, s_actions, s_w, s_rewards, s_next_obs, s_dones, s_inds) = self._sample_batch_experiences()

            s_next_a = self.actor_target(s_next_obs, s_w)
            qf1_next_target = self.qf1_target(s_next_obs, s_next_a, s_w)
            next_q_value = s_rewards + (1 - s_dones.reshape(-1, 1)) * self.gamma * qf1_next_target

        qf1_a_values = self.qf1(s_obs, s_actions, s_w)
        qf1_loss = F.mse_loss(qf1_a_values, next_q_value)

        # optimize the model
        self.q_optimizer.zero_grad()
        qf1_loss.backward()
        self.q_optimizer.step()

        if self.global_step % self.policy_frequency == 0:
            actor_loss = -self.qf1(s_obs, self.actor(s_obs, s_w)).mean()

            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()

            self.prev_actor_loss = actor_loss.item()

            # update the target network
            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
            for param, target_param in zip(self.qf1.parameters(), self.qf1_target.parameters()):
                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
        else:
            actor_loss = None

        if self.log and self.global_step % 100 == 0:
            wandb.log({
                "losses/q_loss": qf1_loss.item(),
                "losses/actor_loss": actor_loss.item() if actor_loss is not None else self.prev_actor_loss,
                "global_step": self.global_step,
            })

        td_err = (qf1_a_values[: len(s_inds)] - qf1_next_target[: len(s_inds)]).detach()
        priority = torch.einsum("sr,sr->s", td_err, s_w[: len(s_inds)]).abs()
        priority = priority.cpu().numpy().flatten()
        priority = (priority + self.replay_buffer.min_priority) ** self.per_alpha
        self.replay_buffer.update_priorities(s_inds, priority)

    def train(
            self,
            total_timesteps: int,
            eval_envs: List[EVCharging],
            ref_point: np.ndarray,
            known_pareto_front: Optional[List[np.ndarray]] = None,
            num_eval_weights_for_front: int = 100,
            num_eval_episodes_for_front: int = 5,
            eval_freq: int = 100000,
            hv_eval_freq: int = 100000,
            reset_num_timesteps: bool = False,
    ):
        """Train the agent."""

        if self.log:
            self.register_additional_config({"ref_point": ref_point.tolist(), "known_front": known_pareto_front})

        eval_weights = equally_spaced_weights(self.reward_dim, n=num_eval_weights_for_front, seed=self.seed)

        self.global_step = 0 if reset_num_timesteps else self.global_step
        self.num_episodes = 0 if reset_num_timesteps else self.num_episodes

        env_iteration = 0
        env = self.envs[0]
        obs, info = env.reset()

        for _ in range(1, total_timesteps + 1):
            # If we've run env_iterations episodes on the current environment, switch to the next one
            if env_iteration >= self.env_iterations:
                # Deterministically select the environment using env_idx
                env = self.rnd.choice(self.envs)
                obs, info = env.reset()
                env_iteration = 0

            self.global_step += 1

            tensor_w = self.weight_sampler.sample(1).view(-1).to(self.device)

            w = tensor_w.detach().cpu().numpy()
            if self.global_step < self.learning_starts:
                action = env.action_space.sample()
            else:
                with torch.no_grad():
                    action = self.actor.get_action(
                        torch.tensor(obs).float().to(self.device),
                        tensor_w,
                    ).detach().cpu().numpy()
                    noise = self.exploration_noise.sample()
                    action += noise
                    action = action.clip(self.action_space.low, self.action_space.high)

            next_obs, vector_reward, terminated, truncated, info = env.step(action)

            self.replay_buffer.add(obs, action, w, vector_reward, next_obs, terminated)

            if self.global_step >= self.learning_starts:
                self.update()

            if terminated:
                env_iteration += 1  # Increment the episode counter
                obs, info = env.reset()
                self.num_episodes += 1

                if self.log and "episode" in info.keys():
                    log_episode_info(info["episode"], np.dot, w, self.global_step)
            else:
                obs = next_obs

        self.global_step += 1

    @torch.no_grad()
    def eval(
            self, obs: Union[np.ndarray, torch.Tensor], w: Union[np.ndarray, torch.Tensor], torch_action=False
    ) -> Union[np.ndarray, torch.Tensor]:
        """Evaluate the policy action for the given observation and weight vector."""
        if isinstance(obs, np.ndarray):
            obs = torch.tensor(obs).float().to(self.device)
            w = torch.tensor(w).float().to(self.device)

        action = self.actor.get_action(obs, w)

        if not torch_action:
            action = action.detach().cpu().numpy()

        return action

    def save(self, save_dir="weights/", filename=None, save_replay_buffer=True):
        """Save the agent's weights and replay buffer."""
        if not os.path.isdir(save_dir):
            os.makedirs(save_dir)
        pass

    def load(self, save_dir="weights/", filename: str = None, load_replay_buffer=True):
        """Load the agent weights from a file."""
        if not os.path.exists(f'{save_dir}/{filename}.tar'):
            raise ValueError(f'No saved model found at {save_dir}/{filename}')
        pass

    def get_config(self):
        """Get the configuration of the agent."""
        return {
            "env_id": self.env.unwrapped.spec.id,
            "learning_rate": self.learning_rate,
            "batch_size": self.batch_size,
            "tau": self.tau,
            "gamma": self.gamma,
            "net_arch": self.net_arch,
            "policy_frequency": self.policy_frequency,
            "buffer_size": self.buffer_size,
            "learning_starts": self.learning_starts,
            "per_alpha": self.per_alpha,
            'env_iterations': self.env_iterations,
            "seed": self.seed,
        }
