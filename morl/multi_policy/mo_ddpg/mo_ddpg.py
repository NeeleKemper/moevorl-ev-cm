# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ddpg/#ddpg_continuous_actionpy
import os
import time
from copy import deepcopy

import wandb
import random
from typing import List, Optional, Union

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from morl.common.prioritized_buffer import PrioritizedReplayBuffer
from envs.ev_charging.ev_charging import EVCharging
from morl.common.evaluation import seed_everything
from morl.common.evaluation import (
    log_all_multi_policy_metrics,
    log_episode_info,
    multi_policy_evaluation
)
from morl.common.weight_sampler import RandomWeightSampler
from morl.common.morl_algorithm import MOAgent, MOPolicy
from morl.common.networks import layer_init, mlp
from morl.common.weights import equally_spaced_weights


class EarlyStopping:
    def __init__(self, patience: int = 5, restarts: int = 3, min_delta: float = 0):
        self.patience = patience
        self.restarts = restarts
        self.min_delta = min_delta
        self.best_metric = float('-inf')
        self.best_q = None
        self.best_actor = None
        self.best_q_target = None
        self.best_actor_target = None
        self.best_q_optimizer = None
        self.best_actor_optimizer = None
        self.patience_counter = 0
        self.restart_counter = 0

    def __call__(self, metric, q, actor, q_target, actor_target, q_optimizer, actor_optimizer):
        if metric - self.min_delta > self.best_metric:
            self.best_metric = metric
            self.best_q = deepcopy(q.state_dict())
            self.best_actor = deepcopy(actor.state_dict())
            self.best_q_target = deepcopy(q_target.state_dict())
            self.best_actor_target = deepcopy(actor_target.state_dict())
            self.best_q_optimizer = deepcopy(q_optimizer.state_dict())
            self.best_actor_optimizer = deepcopy(actor_optimizer.state_dict())
            self.patience_counter = 0
        else:
            self.patience_counter += 1
            if self.patience_counter >= self.patience:
                self.restart_counter += 1
                return True  # Indicate early stopping
        return False

    def do_restart(self):
        if self.restart_counter <= self.restarts:
            self.patience_counter = 0
            return True
        return False

    def load_best_model(self, q, actor, q_target, actor_target, q_optimizer, actor_optimizer):
        if self.best_q is not None:
            q.load_state_dict(self.best_q)
        if self.best_actor is not None:
            actor.load_state_dict(self.best_actor)
        if self.best_q_target is not None:
            q_target.load_state_dict(self.best_q_target)
        if self.best_actor_target is not None:
            actor_target.load_state_dict(self.best_actor_target)
        if self.best_q_optimizer is not None:
            q_optimizer.load_state_dict(self.best_q_optimizer)
        if self.best_actor_optimizer is not None:
            actor_optimizer.load_state_dict(self.best_actor_optimizer)


class OrnsteinUhlenbeckNoise:
    def __init__(self, size: int, theta=0.15, sigma=0.05, ou_noise=0.0, dt=1e-2):
        '''
        The noise of Ornstein-Uhlenbeck Process

        Source: https://github.com/slowbull/DDPG/blob/master/src/explorationnoise.py
        It makes Zero-mean Gaussian Noise more stable.
        It helps agent explore better in a inertial system.
        Don't abuse OU Process. OU process has too much hyper-parameters and over fine-tuning make no sense.

        int size: the size of noise, noise.shape==(-1, action_dim)
        float theta: related to the not independent of OU-noise
        float sigma: related to action noise std
        float ou_noise: initialize OU-noise
        float dt: derivative
        '''
        self.theta = theta
        self.sigma = sigma
        self.ou_noise = ou_noise
        self.dt = dt
        self.size = size

    def sample(self) -> float:
        '''
        output a OU-noise

        return array ou_noise: a noise generated by Ornstein-Uhlenbeck Process
        '''
        noise = self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)
        self.ou_noise -= self.theta * self.ou_noise * self.dt + noise
        return self.ou_noise


class ReplayMemory:
    '''Replay memory.'''

    def __init__(self, capacity: int, seed: int):
        '''Initialize the replay memory.'''
        self.capacity = capacity
        self.buffer = []
        self.position = 0
        self.rnd = random.Random(seed)

    def add(self, state, action, weights, reward, next_state, done):
        '''Push a transition.'''
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        self.buffer[self.position] = (
            np.array(state).copy(),
            np.array(action).copy(),
            np.array(weights).copy(),
            np.array(reward).copy(),
            np.array(next_state).copy(),
            np.array(done).copy(),
        )
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size, to_tensor=True, device=None):
        '''Sample a batch of transitions.'''
        batch = self.rnd.sample(self.buffer, batch_size)
        state, action, w, reward, next_state, done = map(np.stack, zip(*batch))
        experience_tuples = (state, action, w, reward, next_state, done)
        if to_tensor:
            return tuple(map(lambda x: torch.tensor(x, dtype=torch.float32).to(device), experience_tuples))
        return state, action, w, reward, next_state, done

    def reset(self):
        self.buffer = []
        self.position = 0

    def __len__(self):
        '''Return the current size of the buffer.'''
        return len(self.buffer)


# ALGO LOGIC: initialize agent here:
class QNetwork(nn.Module):
    def __init__(self, obs_dim: int, action_dim: int, rew_dim: int, net_arch=[256, 256]):
        super().__init__()
        self.net = mlp(obs_dim + action_dim + rew_dim, rew_dim, net_arch)
        self.apply(layer_init)

    def forward(self, obs, action, w):
        return self.net(torch.cat((obs, action, w), dim=obs.dim() - 1))


class Actor(nn.Module):
    def __init__(self, obs_dim: int, action_dim: int, reward_dim: int, action_space: any, net_arch=[256, 256]):
        super().__init__()
        self.net = mlp(obs_dim + reward_dim, action_dim, net_arch)

        # action rescaling
        self.register_buffer(
            'action_scale', torch.tensor((action_space.high - action_space.low) / 2.0, dtype=torch.float32)
        )
        self.register_buffer(
            'action_bias', torch.tensor((action_space.high + action_space.low) / 2.0, dtype=torch.float32)
        )

    def forward(self, obs, w):
        x = torch.concat((obs, w), dim=obs.dim() - 1)
        x = self.net(x)
        return torch.tanh(x) * self.action_scale + self.action_bias

    def get_action(self, obs, w):
        '''Get an action from the policy network.'''
        return self.forward(obs, w)


class MODDPG(MOAgent, MOPolicy):
    def __init__(
            self,
            envs,
            learning_rate: float = 1e-4,
            gamma: float = 0.99,
            tau: float = 0.005,
            buffer_size: int = 1000000,
            net_arch: List = [512, 512],
            batch_size: int = 256,
            learning_starts: int = 1000,
            per: bool = True,
            per_alpha: float = 0.6,
            policy_frequency: int = 1,
            env_iterations: int = 10,
            project_name: str = 'MORL-Baselines',
            experiment_name: str = 'MO_DDPG',
            wandb_entity: Optional[str] = None,
            log: bool = True,
            seed: Optional[int] = None,
            device: Union[torch.device, str] = 'auto',
    ):
        if seed is not None:
            seed_everything(seed)
        self.rnd = random.Random(seed)

        MOAgent.__init__(self, envs[0], device=device, seed=seed)
        MOPolicy.__init__(self, device=device)
        self.envs = envs
        self.learning_rate = learning_rate
        self.tau = tau
        self.gamma = gamma
        self.buffer_size = buffer_size
        self.net_arch = net_arch
        self.learning_starts = learning_starts
        self.per = per
        self.per_alpha = per_alpha
        self.batch_size = batch_size
        self.policy_frequency = policy_frequency
        self.env_iterations = env_iterations
        self.seed = seed

        # Q network
        self.qf1 = QNetwork(obs_dim=self.observation_dim, action_dim=self.action_dim, rew_dim=self.reward_dim,
                            net_arch=net_arch).to(self.device)
        self.qf1_target = QNetwork(obs_dim=self.observation_dim, action_dim=self.action_dim, rew_dim=self.reward_dim,
                                   net_arch=net_arch).to(self.device)
        self.qf1_target.load_state_dict(self.qf1.state_dict())

        # Actor network
        self.actor = Actor(obs_dim=self.observation_dim, action_dim=self.action_dim, reward_dim=self.reward_dim,
                           action_space=self.action_space, net_arch=net_arch).to(self.device)
        self.actor_target = Actor(obs_dim=self.observation_dim, action_dim=self.action_dim, reward_dim=self.reward_dim,
                                  action_space=self.action_space, net_arch=net_arch).to(self.device)
        self.actor_target.load_state_dict(self.actor.state_dict())

        # optimizer
        self.q_optimizer = optim.Adam(list(self.qf1.parameters()), lr=self.learning_rate)
        self.actor_optimizer = optim.Adam(list(self.actor.parameters()), lr=self.learning_rate)

        # replay buffer
        if not self.per:
            self.replay_buffer = ReplayMemory(self.buffer_size, self.seed)

        else:
            self.replay_buffer = PrioritizedReplayBuffer(self.observation_shape, self.action_dim,
                                                         rew_dim=self.reward_dim,
                                                         max_size=self.buffer_size, obs_dtype=np.float32,
                                                         action_dtype=np.float32, min_priority=1e-5)

        # noise
        self.exploration_noise = OrnsteinUhlenbeckNoise(size=self.action_dim)

        self.early_stopper = EarlyStopping(patience=5, restarts=2, min_delta=0.01)

        # weight sampler
        self.weight_sampler = RandomWeightSampler(self.reward_dim, self.seed)

        # log
        self.log = log
        if self.log:
            self.setup_wandb(project_name, experiment_name, wandb_entity)
        self.save_dir = f'models/moddpg'
        self.prev_actor_loss = 0

    def _sample_batch_experiences(self):
        return self.replay_buffer.sample(self.batch_size, to_tensor=True, device=self.device)

    def update(self):
        with torch.no_grad():
            if not self.per:
                (s_obs, s_actions, s_w, s_rewards, s_next_obs, s_dones) = self._sample_batch_experiences()
            else:
                (s_obs, s_actions, s_w, s_rewards, s_next_obs, s_dones, s_inds) = self._sample_batch_experiences()

            s_next_a = self.actor_target(s_next_obs, s_w)
            qf1_next_target = self.qf1_target(s_next_obs, s_next_a, s_w)
            next_q_value = s_rewards + (1 - s_dones.reshape(-1, 1)) * self.gamma * qf1_next_target

        qf1_a_values = self.qf1(s_obs, s_actions, s_w)
        qf1_loss = F.mse_loss(qf1_a_values, next_q_value)

        # optimize the model
        self.q_optimizer.zero_grad()
        qf1_loss.backward()
        self.q_optimizer.step()

        if self.global_step % self.policy_frequency == 0:
            actor_loss = -self.qf1(s_obs, self.actor(s_obs, s_w), s_w).mean()

            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()

            self.prev_actor_loss = actor_loss.item()

            # update the target network
            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
            for param, target_param in zip(self.qf1.parameters(), self.qf1_target.parameters()):
                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
        else:
            actor_loss = None

        if self.log and self.global_step % 100 == 0:
            wandb.log({
                'losses/q_loss': qf1_loss.item(),
                'losses/actor_loss': actor_loss.item() if actor_loss is not None else self.prev_actor_loss,
                'global_step': self.global_step,
            })

        if self.per:
            td_err = (qf1_a_values[: len(s_inds)] - qf1_next_target[: len(s_inds)]).detach()
            priority = torch.einsum('sr,sr->s', td_err, s_w[: len(s_inds)]).abs()
            priority = priority.cpu().numpy().flatten()
            priority = (priority + self.replay_buffer.min_priority) ** self.per_alpha
            self.replay_buffer.update_priorities(s_inds, priority)

    def train(
            self,
            total_timesteps: int,
            eval_envs: List[EVCharging],
            ref_point: np.ndarray,
            known_pareto_front: Optional[List[np.ndarray]] = None,
            num_eval_weights_for_front: int = 200,
            num_eval_episodes_for_front: int = 5,
            eval_freq: int = 100000,
            hv_eval_freq: int = 100000,
            reset_num_timesteps: bool = False,
            save_file_name: str = 'MODDPG',
            sub_folder: str = 'scenario_CS05',
    ):
        '''Train the agent.'''

        if self.log:
            self.register_additional_config({'ref_point': ref_point.tolist(), 'known_front': known_pareto_front})

        eval_weights = equally_spaced_weights(self.reward_dim, n=num_eval_weights_for_front, seed=self.seed)

        self.global_step = 0 if reset_num_timesteps else self.global_step
        self.num_episodes = 0 if reset_num_timesteps else self.num_episodes

        env_iteration = 0
        env = self.envs[0]
        obs, info = env.reset()

        total_training_time = 0
        for _ in range(1, total_timesteps + 1):
            start_time = time.time()
            # If we've run env_iterations episodes on the current environment, switch to the next one
            if env_iteration >= self.env_iterations:
                # Deterministically select the environment using env_idx
                env = self.rnd.choice(self.envs)
                obs, info = env.reset()
                env_iteration = 0

            self.global_step += 1

            tensor_w = self.weight_sampler.sample(1).view(-1).to(self.device)
            w = tensor_w.detach().cpu().numpy()
            if self.global_step < self.learning_starts:
                action = env.action_space.sample()
            else:
                with torch.no_grad():
                    action = self.actor.get_action(
                        torch.tensor(obs).float().to(self.device),
                        tensor_w,
                    ).detach().cpu().numpy()
                    noise = self.exploration_noise.sample()
                    action += noise
                    action = action.clip(self.action_space.low, self.action_space.high)

            next_obs, vector_reward, terminated, truncated, info = env.step(action)

            self.replay_buffer.add(obs, action, w, vector_reward, next_obs, terminated)

            if self.global_step >= self.learning_starts:
                self.update()

            if terminated:
                env_iteration += 1  # Increment the episode counter
                obs, info = env.reset()
                self.num_episodes += 1

                if self.log and "episode" in info.keys():
                    log_episode_info(info["episode"], np.dot, w, self.global_step)
            else:
                obs = next_obs

            end_time = time.time()
            total_training_time += (end_time - start_time)

            if eval_envs is not None and self.log:
                if self.global_step % hv_eval_freq == 0:
                    # Evaluation
                    avg_scalarized_return, avg_scalarized_discounted_return, avg_vec_return, avg_disc_vec_return, \
                        avg_constraint_violations, front = \
                        multi_policy_evaluation(self, eval_envs, w=eval_weights, rep=num_eval_episodes_for_front)

                    self.report(
                        avg_scalarized_return,
                        avg_scalarized_discounted_return,
                        avg_vec_return,
                        avg_disc_vec_return,
                        avg_constraint_violations,
                        log_name='eval',
                    )
                    wandb.log({'num_episodes': self.num_episodes, 'training_time': round(total_training_time)})

                    log_all_multi_policy_metrics(
                        current_front=front,
                        hv_ref_point=ref_point,
                        reward_dim=self.reward_dim,
                        global_step=self.global_step,
                        ref_front=known_pareto_front,
                    )

                    hypervolume = wandb.run.summary['eval/hypervolume']
                    if self.early_stopper(hypervolume, q=self.qf1, actor=self.actor, q_target=self.qf1_target,
                                          actor_target=self.actor_target, q_optimizer=self.q_optimizer,
                                          actor_optimizer=self.actor_optimizer):
                        if self.early_stopper.do_restart():
                            self.learning_rate *= 0.1
                            hv_eval_freq = int(hv_eval_freq / 2)
                            eval_freq = int(eval_freq / 2)
                            self.early_stopper.load_best_model(q=self.qf1, actor=self.actor, q_target=self.qf1_target,
                                                               actor_target=self.actor_target,
                                                               q_optimizer=self.q_optimizer,
                                                               actor_optimizer=self.actor_optimizer)
                        else:
                            print('Early stopping triggered')
                            break
                    obs, info = env.reset()

                elif self.global_step % eval_freq == 0:
                    eval_weights_temp = random.choices(eval_weights, k=int(num_eval_weights_for_front * 0.1))

                    avg_scalarized_return, avg_scalarized_discounted_return, avg_vec_return, avg_disc_vec_return, \
                        avg_constraint_violations, front = \
                        multi_policy_evaluation(self, eval_envs, w=eval_weights_temp, rep=num_eval_episodes_for_front)
                    self.report(
                        avg_scalarized_return,
                        avg_scalarized_discounted_return,
                        avg_vec_return,
                        avg_disc_vec_return,
                        avg_constraint_violations,
                        log_name='eval',
                    )
                    wandb.log({'num_episodes': self.num_episodes, 'training_time': round(total_training_time)})

        self.global_step += 1
        self.early_stopper.load_best_model(q=self.qf1, actor=self.actor, q_target=self.qf1_target,
                                           actor_target=self.actor_target,
                                           q_optimizer=self.q_optimizer,
                                           actor_optimizer=self.actor_optimizer)

        if eval_envs is not None and self.log:
            # Evaluation
            avg_scalarized_return, avg_scalarized_discounted_return, avg_vec_return, avg_disc_vec_return, \
                avg_constraint_violations, front = \
                multi_policy_evaluation(self, eval_envs, w=eval_weights, rep=num_eval_episodes_for_front)

            self.report(
                avg_scalarized_return,
                avg_scalarized_discounted_return,
                avg_vec_return,
                avg_disc_vec_return,
                avg_constraint_violations,
                log_name='eval',
            )

            log_all_multi_policy_metrics(
                current_front=front,
                hv_ref_point=ref_point,
                reward_dim=self.reward_dim,
                global_step=self.global_step,
                ref_front=known_pareto_front,
            )

        self.save(sub_folder=sub_folder, filename=save_file_name, save_replay_buffer=False)
        hypervolume = wandb.run.summary['eval/hypervolume']
        self.close_wandb()
        return hypervolume

    @torch.no_grad()
    def eval(
            self, obs: Union[np.ndarray, torch.Tensor], w: Union[np.ndarray, torch.Tensor], torch_action=False
    ) -> Union[np.ndarray, torch.Tensor]:
        '''Evaluate the policy action for the given observation and weight vector.'''
        if isinstance(obs, np.ndarray):
            obs = torch.tensor(obs).float().to(self.device)
            w = torch.tensor(w).float().to(self.device)

        action = self.actor.get_action(obs, w)

        if not torch_action:
            action = action.detach().cpu().numpy()

        return action

    def save(self, sub_folder: str = None, filename: str = None, save_replay_buffer=False):
        save_dir = f'{self.save_dir}/{sub_folder}'
        if not os.path.isdir(save_dir):
            os.makedirs(save_dir)
        save_path = os.path.join(save_dir, f'{filename}.tar')
        save_dict = {
            'actor_state_dict': self.actor.state_dict(),
            'actor_target_state_dict': self.actor_target.state_dict(),
            'qf1_state_dict': self.qf1.state_dict(),
            'qf1_target_state_dict': self.qf1_target.state_dict(),
            'actor_optimizer': self.actor_optimizer.state_dict(),
            'q_optimizer': self.q_optimizer.state_dict(),
            'replay_buffer': self.replay_buffer if save_replay_buffer else None
        }
        torch.save(save_dict, save_path)
        print(f'Model saved to {save_path}')

    def load(self, sub_folder: str = None, filename: str = None, load_replay_buffer=False):
        save_dir = f'{self.save_dir}/{sub_folder}'
        load_path = os.path.join(save_dir, f'{filename}.tar')
        if not os.path.exists(load_path):
            raise ValueError(f'No saved model found at {save_dir}/{filename}')
        if torch.cuda.is_available():
            load_dict = torch.load(load_path)
        else:
            load_dict = torch.load(load_path, map_location=torch.device('cpu'))
        self.actor.load_state_dict(load_dict['actor_state_dict'])
        self.actor_target.load_state_dict(load_dict['actor_target_state_dict'])
        self.qf1.load_state_dict(load_dict['qf1_state_dict'])
        self.qf1_target.load_state_dict(load_dict['qf1_target_state_dict'])
        self.actor_optimizer.load_state_dict(load_dict['actor_optimizer'])
        self.q_optimizer.load_state_dict(load_dict['q_optimizer'])
        if load_replay_buffer and 'replay_buffer' in load_dict:
            self.replay_buffer = load_dict['replay_buffer']
        print(f'Model loaded from {load_path}')

    def get_config(self):
        return {
            'env_id': self.env.unwrapped.spec.id,
            'learning_rate': self.learning_rate,
            'batch_size': self.batch_size,
            'tau': self.tau,
            'gamma': self.gamma,
            'net_arch': self.net_arch,
            'policy_frequency': self.policy_frequency,
            'buffer_size': self.buffer_size,
            'learning_starts': self.learning_starts,
            'per_alpha': self.per_alpha,
            'env_iterations': self.env_iterations,
            'seed': self.seed,
        }

    def get_weights(self, num_eval_weights_for_front: int = 200):
        return equally_spaced_weights(self.reward_dim, n=num_eval_weights_for_front, seed=self.seed)

